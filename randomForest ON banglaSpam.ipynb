{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b887bd-0505-4256-a1e3-28d7a3e5b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c72192-0c48-48d2-9185-22a49678200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV file\n",
    "df = pd.read_csv('processed_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1892b6fe-bb5a-4c56-8d49-4c16ae1bbd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nSample of the dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc71f62-0490-4b03-83b7-4c8ae0d054b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e535c-dee4-478a-a5b2-d84372aba4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in 'type' column\n",
    "print(\"\\nUnique values in 'type' column:\")\n",
    "print(df['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38533e1a-92c4-45b3-9817-831f63988e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the type values - treat case insensitively and NaN as spam\n",
    "def standardize_type(x):\n",
    "    if pd.isna(x):  # Handle NaN values as spam\n",
    "        return 'spam'\n",
    "\n",
    "    if not isinstance(x, str):\n",
    "        x = str(x)  # Convert non-string types to string\n",
    "\n",
    "    x_lower = x.lower().strip()\n",
    "\n",
    "    # Exact match for main categories\n",
    "    if 'spam' in x_lower:\n",
    "        return 'spam'\n",
    "    elif 'ham' in x_lower:\n",
    "        return 'ham'\n",
    "    elif 'promo' in x_lower:\n",
    "        return 'promo'\n",
    "    else:\n",
    "        return x_lower  # Return as is for other categories\n",
    "\n",
    "df['standardized_type'] = df['type'].apply(standardize_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0001d61b-ed5c-4b45-aa38-743167c6f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display distribution of standardized types\n",
    "print(\"\\nStandardized type distribution:\")\n",
    "print(df['standardized_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e4b475-95a8-444a-820f-ec36b3751b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder for the target variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['type_encoded'] = label_encoder.fit_transform(df['standardized_type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f578e36f-bd23-4f82-b5cf-8029483f07d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the encoded values back to their original labels for reference\n",
    "encoded_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"\\nEncoded class mapping:\")\n",
    "for class_name, encoded_value in encoded_mapping.items():\n",
    "    print(f\"{class_name} -> {encoded_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab586f29-eb28-48fc-8238-c48711737597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (all columns except type-related and text columns)\n",
    "features = [col for col in df.columns if col not in ['type', 'text', 'standardized_type', 'type_encoded', 'label']]\n",
    "print(\"\\nFeatures used for classification:\")\n",
    "print(features)\n",
    "\n",
    "# Prepare features and target\n",
    "X = df[features]\n",
    "y = df['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259dfbd7-bbc4-483e-979f-7977d8744d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106e00a-d54d-47b8-8da0-479cedf40e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# First, create copies to avoid modifying the original data\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Apply standard scaler only to the numerical columns (avg_word_length and word_length)\n",
    "# This will scale them to have a mean of 0 and standard deviation of 1\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Identify the columns to scale\n",
    "columns_to_scale = ['avg_word_length', 'word_length']\n",
    "# Fit the scaler on the training data and transform both training and test data\n",
    "X_train_scaled[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])\n",
    "X_test_scaled[columns_to_scale] = scaler.transform(X_test[columns_to_scale])\n",
    "\n",
    "# To make the values be between 0 and 1 instead of standardized,\n",
    "# we can use MinMaxScaler instead of StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Apply MinMaxScaler to scale to 0-1 range\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train_scaled[columns_to_scale] = min_max_scaler.fit_transform(X_train[columns_to_scale])\n",
    "X_test_scaled[columns_to_scale] = min_max_scaler.transform(X_test[columns_to_scale])\n",
    "\n",
    "# Verify the scaled values are between 0 and 1\n",
    "print(\"\\nScaled training data sample (avg_word_length and word_length should be between 0-1):\")\n",
    "print(X_train_scaled[columns_to_scale].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e2064a-07d7-4a43-9676-f2e2dce6eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train a Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on both training and test sets\n",
    "y_train_pred = rf_classifier.predict(X_train_scaled)\n",
    "y_test_pred = rf_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nRandom Forest Classifier Results:\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Print detailed classification report for test set\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Ham','Spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509705db-73db-4bfd-b58c-66216c356631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix for test set\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "# Feature importance\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_classifier.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ad1e3-99be-493a-b2f6-be214066bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "print(\"\\nTraining Random Forest model...\")\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Drop problematic columns\n",
    "X_train_numeric = X_train.select_dtypes(include=['int64', 'float64'])\n",
    "X_test_numeric = X_test.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Train with only numeric features\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_numeric, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b61c5-5ce2-4d7a-a05e-f9ef544fec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on training data\n",
    "train_preds = rf.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "print(f\"\\nTraining accuracy: {train_accuracy:.3f}\")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Create preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline that includes preprocessing and the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Now you can use the pipeline for prediction\n",
    "train_preds = pipeline.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "print(f\"\\nTraining accuracy: {train_accuracy:.3f}\")\n",
    "\n",
    "# And for test data when ready\n",
    "# test_preds = pipeline.predict(X_test)\n",
    "# test_accuracy = accuracy_score(y_test, test_preds)\n",
    "# print(f\"Test accuracy: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94da3ea1-69ae-4624-a231-29cabd890acc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate model on test data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_preds \u001b[38;5;241m=\u001b[39m \u001b[43mrf\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      3\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, test_preds)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rf' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test data\n",
    "test_preds = rf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_preds)\n",
    "print(f\"Test accuracy: {test_accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df35774-ffbd-48f3-be66-6979d4f40fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original class names for display\n",
    "original_class_names = label_encoder.classes_\n",
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, test_preds, target_names=original_class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18634da7-d65a-4685-a7a6-18aa964d785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "cm = confusion_matrix(y_test, test_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=original_class_names,\n",
    "            yticklabels=original_class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_multiclass.png')\n",
    "print(\"\\nConfusion matrix saved as 'confusion_matrix_multiclass.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2843e0-4f5b-4eeb-b7df-50763472637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': rf.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_multiclass.png')\n",
    "print(\"Feature importance plot saved as 'feature_importance_multiclass.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acffb46-ebc8-4466-8a96-4a9dcd5d176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 5 most important features\n",
    "print(\"\\nTop 5 most important features:\")\n",
    "print(feature_importance.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e941c8dd-1bf4-4f3f-a8c0-62605c4bf25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters():\n",
    "    print(\"\\nPerforming hyperparameter tuning...\")\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "    # Train with best parameters\n",
    "    best_rf = RandomForestClassifier(**grid_search.best_params_, random_state=42)\n",
    "    best_rf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    best_preds = best_rf.predict(X_test)\n",
    "    best_accuracy = accuracy_score(y_test, best_preds)\n",
    "    print(f\"Tuned model test accuracy: {best_accuracy:.3f}\")\n",
    "\n",
    "    return best_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d6b88d-e4f9-4089-b18a-1f6403f2c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and label encoder\n",
    "import pickle\n",
    "with open('message_classifier_model.pkl', 'wb') as file:\n",
    "    pickle.dump({'model': rf, 'label_encoder': label_encoder, 'features': features}, file)\n",
    "print(\"\\nModel saved as 'message_classifier_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f4567c-2eb5-4598-bd8d-698edbdcc419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on new data\n",
    "def predict_message_type(model_data, new_data):\n",
    "    # Extract model components\n",
    "    model = model_data['model']\n",
    "    label_encoder = model_data['label_encoder']\n",
    "    features = model_data['features']\n",
    "\n",
    "    # Ensure new_data has the same features as training data\n",
    "    needed_features = [f for f in features if f in new_data.columns]\n",
    "    missing_features = [f for f in features if f not in new_data.columns]\n",
    "\n",
    "    if missing_features:\n",
    "        print(f\"Warning: Missing features: {missing_features}\")\n",
    "        for feature in missing_features:\n",
    "            new_data[feature] = 0  # Add missing features with default values\n",
    "\n",
    "    new_data_features = new_data[features]\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_encoded = model.predict(new_data_features)\n",
    "    predictions = label_encoder.inverse_transform(predictions_encoded)\n",
    "\n",
    "    # Get probabilities for each class\n",
    "    probabilities = model.predict_proba(new_data_features)\n",
    "\n",
    "    # Create results DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'Predicted_Type': predictions\n",
    "    })\n",
    "\n",
    "    # Add probability columns for each class\n",
    "    for i, class_name in enumerate(label_encoder.classes_):\n",
    "        results[f'{class_name}_Probability'] = probabilities[:, i]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da8e0a1-e8c5-4c0c-a5f7-aa5cf0c761b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nModel training and evaluation complete!\")\n",
    "print(\"To use this model for predictions:\")\n",
    "print(\"1. Load the model: model_data = pickle.load(open('message_classifier_model.pkl', 'rb'))\")\n",
    "print(\"2. Call: predict_message_type(model_data, new_data)\")\n",
    "# Add a simple test prediction example\n",
    "print(\"\\nExample prediction code:\")\n",
    "print(\"import pickle\")\n",
    "print(\"model_data = pickle.load(open('message_classifier_model.pkl', 'rb'))\")\n",
    "print(\"# Create a sample input with same columns as the training data\")\n",
    "print(\"sample_data = pd.DataFrame({\")\n",
    "print(\"    'has_phone_number': [1, 0, 0],\")\n",
    "print(\"    'has_special_chars': [1, 1, 1],\")\n",
    "print(\"    # Add other features...\")\n",
    "print(\"})\")\n",
    "print(\"predictions = predict_message_type(model_data, sample_data)\")\n",
    "print(\"print(predictions)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
